header: |
    # Copyright 2018 The IML Authors. All Rights Reserved.
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    # ============================================================================
    #
    # THIS IS A GENERATED DOCKERFILE.
    #
    # This file was assembled from multiple pieces, whose use is documented
    # throughout. Please refer to the IML dockerfiles documentation
    # for more information.

# A combinatorial explosion of Docker images and Dockerfiles.
# Each "release" defines all of the ways to combine related but separate chunks
# of functionality ("slices") by listing all of the "slice sets" to use when
# building.
#
# For example, a release that uses {nightly}{py} would create 4 Dockerfiles
# (which could become images or concrete Dockerfiles), because the "nightly"
# and "py" slice sets both have two entries:
#
#   - nightly (no -py2 because the Python 2 slice set has add_to_name: ""
#   - nightly-py3
#   - nightly-gpu (similar)
#   - nightly-gpu-py3
#
# Releases are all treated differently by TensorFlow's CI systems.
releases:
    # IML
    iml:
        is_dockerfiles: true
        upload_images: false
        tag_specs:
            - "{ubuntu-devel-iml}"

    iml-cuda-10-1:
      is_dockerfiles: true
      upload_images: false
      tag_specs:
        - "{ubuntu-devel-iml-cuda-10-1}"

    iml-rocm:
        is_dockerfiles: true
        upload_images: false
        tag_specs:
            - "{ubuntu-devel-iml-rocm}"

    v1.3.1:
      is_dockerfiles: true
      upload_images: false
      tag_specs:
      - "{ubuntu-devel-v1.3.1}"

    iml-rocm-upstream:
      is_dockerfiles: true
      upload_images: false
      tag_specs:
      - "{ubuntu-devel-iml-rocm-upstream}"

    rocm:
      is_dockerfiles: true
      upload_images: false
      tag_specs:
      - "{rocm}"

slice_sets:

    py:
        - add_to_name: ""
          args:
              - USE_PYTHON_3_NOT_2=
        - add_to_name: "-py3"
          args:
              - USE_PYTHON_3_NOT_2=1

    jupyter:
        - add_to_name: ""
        - add_to_name: "-jupyter"
          partials:
              - jupyter

    ubuntu:
        - add_to_name: ""
          dockerfile_exclusive_name: "cpu"
          partials:
              - ubuntu/version
              - ubuntu/cpu
              - ubuntu/python
              - tensorflow
              - shell
        - add_to_name: "-gpu"
          dockerfile_exclusive_name: "gpu"
          args:
              - TF_PACKAGE=tensorflow-gpu
          partials:
              - ubuntu/version
              - ubuntu/nvidia
              - ubuntu/python
              - tensorflow
              - shell
          tests:
              - import-gpu.sh
          test_runtime: nvidia

    ubuntu-devel:
        - add_to_name: "devel"
          dockerfile_exclusive_name: "devel-cpu"
          partials:
              - ubuntu/version
              - ubuntu/devel-cpu
              - ubuntu/python
              - ubuntu/bazel
              - shell
          tests:
              - build-cpu.sh
          args:
              - CHECKOUT_TF_SRC=1
        - add_to_name: "devel-gpu"
          dockerfile_exclusive_name: "devel-gpu"
          partials:
              - ubuntu/version
              - ubuntu/devel-nvidia
              - ubuntu/python
              - ubuntu/bazel
              - shell
          tests:
              - build-gpu.sh
          test_runtime: nvidia
          args:
              - CHECKOUT_TF_SRC=1

    ubuntu-devel-iml:
        #
        # IML profiler:
        # Build "patched" tensorflow, then run end-to-end Pong training test.
        #
        - add_to_name: "devel-iml-gpu-cuda"
          dockerfile_exclusive_name: "devel-iml-gpu-cuda"
          partials:
          - ubuntu/version
          - ubuntu/devel-nvidia
          - ubuntu/python
          - ubuntu/bazel
          - ubuntu/nvidia_nccl
          - ubuntu/nvidia_rtx_2070_env
          - ubuntu/add_user
          - ubuntu/pip_download_tf_v1.13.1
          - ubuntu/openmpi_from_src
          - ubuntu/nodejs
          # - ubuntu/python36_from_src
          - ubuntu/virtualenv
          - ubuntu/pip_dependencies
          - ubuntu/protobuf_from_src
          - ubuntu/cmake_from_src
          - iml/env
          - iml/apt_install
          - iml/pip_dependencies
          - iml/user_custom
          - shell
          - ubuntu/bazelisk
          - ubuntu/entrypoint
          test_runtime: nvidia
          args:
          # NOTE: must be provided at assembler.py command line
          - CHECKOUT_TF_SRC=0
          - USE_PYTHON_3_NOT_2=1
          run_args:
          # NOTE: To make it so we can run the same scripts inside/outside containers:
          # - Outside containers: we define these environment variables in a source_me.sh
          # - Inside containers: we require these to be defined when calling assembler.py (docker run)
          #
          # The root directory of a 'patched' TensorFlow checkout.
          - TENSORFLOW_DIR=
          # The root directory of the iml_profiler repo checkout.
          - IML_DIR=
          # The root directory of the iml_drill repo checkout.
          - IML_DRILL_DIR=
          # The local path where we should output bazel objects (overrides $HOME/.cache/bazel).
          - BAZEL_BUILD_DIR=

    ubuntu-devel-iml-cuda-10-1:
      #
      # IML profiler:
      # Build "patched" tensorflow, then run end-to-end Pong training test.
      #
      - add_to_name: "devel-iml-gpu-cuda-10-1"
        dockerfile_exclusive_name: "devel-iml-gpu-cuda-10-1"
        partials:
#          - ubuntu/version
          - ubuntu/devel-nvidia-cuda-10-1
          - ubuntu/python
          - ubuntu/bazel
          - ubuntu/nvidia_nccl
          - ubuntu/nvidia_rtx_2070_env
          - ubuntu/add_user
          - ubuntu/pip_download_tf_v1.13.1
          - ubuntu/openmpi_from_src
          - ubuntu/nodejs
          - ubuntu/python36_from_src
          - ubuntu/virtualenv
          - ubuntu/pip_dependencies
          - ubuntu/protobuf_from_src
          - ubuntu/cmake_from_src
          - iml/env
          - iml/apt_install
          - iml/pip_dependencies
          - shell
          - iml/user_custom
          - ubuntu/cublas-workaround-cuda-10-1
          - ubuntu/bazelisk
          - ubuntu/entrypoint
        test_runtime: nvidia
        args:
          # NOTE: must be provided at assembler.py command line
          - CHECKOUT_TF_SRC=0
          - USE_PYTHON_3_NOT_2=1
        run_args:
          # NOTE: To make it so we can run the same scripts inside/outside containers:
          # - Outside containers: we define these environment variables in a source_me.sh
          # - Inside containers: we require these to be defined when calling assembler.py (docker run)
          #
          # The root directory of a 'patched' TensorFlow checkout.
          - TENSORFLOW_DIR=
          # The root directory of the iml_profiler repo checkout.
          - IML_DIR=
          # The root directory of the iml_drill repo checkout.
          - IML_DRILL_DIR=
          # The local path where we should output bazel objects (overrides $HOME/.cache/bazel).
          - BAZEL_BUILD_DIR=

    ubuntu-devel-iml-rocm:
        - add_to_name: "devel-iml-gpu-rocm"
          dockerfile_exclusive_name: "devel-iml-gpu-rocm"
          partials:
          - ubuntu/version
          - ubuntu/devel-rocm
          - ubuntu/python
          - ubuntu/bazel
          - ubuntu/amd_rocm_env
          - ubuntu/rocm_tf_dependencies
          - ubuntu/add_user
          - shell
          - ubuntu/virtualenv
          - ubuntu/pip_dependencies
          - ubuntu/rocm_path
          - ubuntu/rocm_hip_env
          test_runtime: rocm
          args:
          - CHECKOUT_TF_SRC=0
          - USE_PYTHON_3_NOT_2=1
          run_args:
          - TENSORFLOW_DIR=
          - IML_DIR=
          - IML_DRILL_DIR=
          - BAZEL_BUILD_DIR=

    ubuntu-devel-iml-rocm-upstream:
    - add_to_name: "devel-iml-gpu-rocm-upstream"
      dockerfile_exclusive_name: "devel-iml-gpu-rocm-upstream"
      partials:
      - ubuntu/version
      - ubuntu/devel-rocm
      - ubuntu/python
      - ubuntu/bazel
      - ubuntu/amd_rocm_env
      - ubuntu/rocm_tf_dependencies
      - ubuntu/add_user
      - shell
      - ubuntu/virtualenv
      - ubuntu/pip_dependencies
      - ubuntu/rocm_path
      - ubuntu/rocm_hip_env
      test_runtime: rocm
      args:
      # Indicates this repo is a checkout from ROCm's repo:
      # git clone -b r1.13-rocm https://github.com/ROCmSoftwarePlatform/tensorflow-upstream.git
      # The repo includes a "build_rocm_python3" script we can run.
      - ROCM_UPSTREAM_TF=1
      - CHECKOUT_TF_SRC=0
      - USE_PYTHON_3_NOT_2=1
      run_args:
      - TENSORFLOW_DIR=
      - IML_DIR=
      - IML_DRILL_DIR=
      - BAZEL_BUILD_DIR=

    rocm:
    - add_to_name: "rocm"
      dockerfile_exclusive_name: "rocm"
      partials:
      - ubuntu/version
      - ubuntu/devel-rocm
#      - ubuntu/python
#      - ubuntu/bazel
      - ubuntu/amd_rocm_env
#      - ubuntu/add_user
      - shell
#      - ubuntu/virtualenv
      - ubuntu/rocm_path
      test_runtime: rocm
      args:
      - CHECKOUT_TF_SRC=0
      - USE_PYTHON_3_NOT_2=1
      run_args:
      - TENSORFLOW_DIR=
      - IML_DIR=
      - IML_DRILL_DIR=
      - BAZEL_BUILD_DIR=

    nightly:
        - add_to_name: "nightly"
          partials:
              - ubuntu/version
              - ubuntu/cpu
              - ubuntu/python
              - tensorflow
              - shell
          args:
              - TF_PACKAGE=tf-nightly
          tests:
              - import.sh
        - add_to_name: "nightly-gpu"
          partials:
              - ubuntu/version
              - ubuntu/nvidia
              - ubuntu/python
              - tensorflow
              - shell
          test_runtime: nvidia
          tests:
              - import-gpu.sh
          args:
              - TF_PACKAGE=tf-nightly-gpu
