syntax = "proto3";

package rlscope;

enum CudaEventType {
    UNKNOWN = 0;
    KERNEL = 1;
    MEMCPY = 2;
    MEMSET = 3;
}

// A single event on a GPU (e.g. memcpy, cudaLaunchKernel).
message CUDAEventProto {
    string name = 1;
    CudaEventType cuda_event_type = 2;
    int64 start_time_us = 3;
    int64 duration_us = 4;
}

message OpStackProto {
    string process_name = 1;
    string phase = 2;
    string machine_name = 3;

    // IMLOperation op_stack = 4;

    // overhead-type -> phase-name -> operation-name -> # of overhead events
    map<string, PhaseOverheadEvents> overhead_events = 4;
}
message PhaseOverheadEvents {
    // phase-name -> operation-name -> # of overhead events
    map<string, OperationOverheadEvents> phase_overhead_events = 1;
}
message OperationOverheadEvents {
    string operation_name = 1;
    int64 num_overhead_events = 2;
}
//message IMLOperation {
//    string name = 1;
//}

// Events belonging to a GPU.
message DevEventsProto {
    string device_name = 1;
    repeated CUDAEventProto events = 2;
}

message MachineDevsEventsProto {
    string process_name = 1;
    string phase = 2;
    string machine_name = 3;

    // One for each device_name.
//    repeated DevEventsProto events = 4;
    map<string, DevEventsProto> dev_events = 4;
}


//
// Collect stats about CUDA runtime/driver API calls.
//
message CUDAAPIThreadStatsProto {
    // POSIX thread_id
    int64 tid = 1;
    // e.g. cudaLaunchKernel
    string api_name = 2;
    int64 total_time_us = 3;
    int64 num_calls = 4;
}

message CUDAAPIEvent {
    // POSIX thread_id
    int64 tid = 1;
    // e.g. cudaLaunchKernel
    string api_name = 2;
    int64 start_time_us = 3;
    int64 duration_us = 4;
    // The operation that was active at the top of the operation stack when this CUDA API was called.
    // We use this to determine where to subtract CUDA API profiling overheads (interception, CUPTI).
    // e.g. q_forward
    string active_operation = 5;
}

// cuda_api_stats*.proto
message CUDAAPIPhaseStatsProto {
    string process_name = 1;
    string machine_name = 2;
    string phase = 3;
    repeated CUDAAPIThreadStatsProto stats = 4;
    // Optional: raw start/end timestamps of CUDA API calls.
    repeated CUDAAPIEvent events = 5;
}

//
// PC sampling
//

message SampleEventsProto {
    string process_name = 1;
    string phase = 2;
    string machine_name = 3;

    repeated SampleEventProto events = 4;
}

message AnnotationProto {
    // "sgd_updates", "training_loop", ...
    string name = 1;
    // "Operation", "Framework API", "Python", ...
    string category = 2;
}

// A SamplingEvent captures all of the sampling-state, at a particular point in time.
//
// Ideally, we capture the sampling-state of each thread on the machine.
message SampleEventProto {
    int64 sample_time_us = 1;
    repeated CPUSampleStateProto cpu_sample_state = 2;
    repeated GPUSampleStateProto gpu_sample_state = 3;
}

message CPUSampleStateProto {
    // CPU name
    string device_name = 1;
    // POSIX thread_id
    int64 tid = 2;
    // 0-based IML API assigned thread_id.
    int64 thread_id = 3;
    // The current active "stack" of annotations.
    repeated AnnotationProto annotations = 4;
}

message GPUSampleStateProto {
    string device_name = 1;
    // Currently, we JUST record whether a GPU kernel is running.
    // Ideally, in the future we would like to collect more GPU-side information.
    // Collecting additional GPU information (e.g. hardware counters) will require
    // the kernel-replay feature which CUDA provides for collecting multiple hardware
    // counters in separate runs (since register space is a limitation).
    bool is_gpu_active = 2;
}

// Q: How do we know if the GPU is active...?
// PC sampling gives us events about GPU activity.
// However, currently we don't have set_operation/end_operation for when the GPU is active.
// Ideally, we would have this so that we know when the GPU is active.
//
// Q: Can we add these annotations somehow?
// - Let cuda_sampling_freq_sec = How often we sample GPU duing PC sampling when the GPU is active.
// extra_time_between_pc_sampling_callbacks_sec = [ average time between PC sampling callbacks ] - cuda_sampling_freq_sec
// fudge_factor_sec = 2*extra_time_between_pc_sampling_callbacks_sec
// def CUDA PC sampling callback:
//   # PC sampling event for GPU;
//   # mark GPU as active, and the last time we saw it active.
//   sampling_state.gpu_active = True
//   sampling_state.start_gpu_active_usec = now_usec()
//
//   # If this is the last PC sampling event we see, we need to set a
//   # "timeout", after which the GPU is considered "inactive".
//   schedule callback in cuda_sampling_freq_sec + fudge_factor_sec seconds:
//     callback:
//       if sampling_state.gpu_active and
//          now_usec() - sampling_state.start_gpu_active_usec > cuda_sampling_freq_sec:
//         # The GPU was last seen as active over cuda_sampling_freq_sec seconds ago,
//         # but has not been marked as active since.
//         # Hence we consider it inactive.
//         sampling_state.gpu_active = False
//         sampling_state.start_gpu_active_usec = now_usec()

