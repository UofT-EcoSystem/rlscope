"""
iml-bench script for running lots of different experiments/benchmarks.
"""
import re
import json
import shlex
from glob import glob
import copy
from glob import glob
import logging
import subprocess
import sys
import os
import traceback
import pandas as pd
import ipdb
import argparse
import pprint
import textwrap
import multiprocessing
import importlib
import gym
# TODO: remove hard dependency on pybullet_envs;
# i.e. we need it for:
#   $ iml-bench stable-baselines --mode [run|all]
# but we don't need pybullet for:
#   $ iml-bench stable-baselines --mode plot
# (assuming all the data is there...)
try:
    import pybullet_envs
except ImportError:
    print(textwrap.dedent("""\
    ERROR: You need to use pip to install pybullet to run iml-bench:
    $ pip install pybullet==2.5.1
    """.rstrip()))
    sys.exit(1)
    pybullet_envs = None

try:
    import atari_py
except ImportError:
    print(textwrap.dedent("""\
    ERROR: You need to use pip to install atari-py to run iml-bench:
    $ pip install git+https://github.com/openai/atari-py.git@0.2.0
    """.rstrip()))
    sys.exit(1)

from os.path import join as _j, abspath as _a, dirname as _d, exists as _e, basename as _b

from iml_profiler.experiment import expr_config
from iml_profiler.profiler import iml_logging
from iml_profiler.profiler.concurrent import ForkedProcessPool, FailedProcessException
from iml_profiler.experiment.util import tee

from iml_profiler.profiler.util import args_to_cmdline

from iml_profiler.parser.common import *

# DEFAULT_IML_TRACE_TIME_SEC = 60*2
# DEFAULT_DEBUG_IML_TRACE_TIME_SEC = 20

MODES = [
    'train_stable_baselines.sh',
    'inference_stable_baselines.sh',
]

STABLE_BASELINES_AVAIL_ENV_IDS = None
STABLE_BASELINES_UNAVAIL_ENV_IDS = None

def add_config_options(pars):

    pars.add_argument(
        '--config',
        default='instrumented',
        # choices=[
        #     'instrumented',
        #     'instrumented_no_tfprof',
        #     'instrumented_no_pyprof',
        #
        #     # Just measure tfprof trace-collection overhead (e.g. libcupti).
        #     'instrumented_no_pyprof_no_tfdump',
        #     # Just measure pyprof trace-collection overhead (i.e. python timestamps for wrapped TensorFlow API calls).
        #     'instrumented_no_tfprof_no_pydump',
        #     # Just measure small aspects of pyprof trace-collection overhead.
        #     'instrumented_no_tfprof_no_pydump_no_pytrace',
        #
        #     # Sanity check: try --iml-disable-tfprof and --iml-disable-pyprof...performance SHOULD match --iml-disableâ€¦
        #     # RESULT: holds true... Really though...pyprof is THAT BAD?
        #     # TODO: disable pyprof dumping, JUST do collection
        #     'instrumented_no_tfprof_no_pyprof',
        #
        #     'uninstrumented',
        #
        #     'instrumented_full',
        #     'instrumented_full_no_tfprof',
        #     'instrumented_full_no_pyprof',
        #     'instrumented_full_no_tfprof_no_pyprof',
        #
        #     'uninstrumented_full'],
        help=textwrap.dedent("""
        instrumented:
            Run the script with IML enabled for the entire duration of training (if --iml-trace-time-sec), 
            record start/end timestamps.
            $ train.py --iml-directory ...
            # NOTE: don't use --iml-trace-time-sec here, we want to collect traces for the whole run
            
        uninstrumented:
            Run the script with IML disabled, record start/end timestamp at start/end of training script
            $ train.py --iml-disable
            
        instrumented_full:
            Run for the ENTIRE training duration (don't stop early)
            
        uninstrumented_full:
            Run for the ENTIRE training duration (don't stop early)
        """))

def add_stable_baselines_options(pars):
    pars.add_argument(
        '--algo',
        choices=expr_config.STABLE_BASELINES_ANNOTATED_ALGOS,
        help='algorithm to run')
    pars.add_argument(
        '--env-id',
        choices=expr_config.STABLE_BASELINES_ENV_IDS,
        help='environment to run')
    pars.add_argument(
        '--bullet',
        action='store_true',
        help='Limit environments to physics-based Bullet environments')
    pars.add_argument(
        '--atari',
        action='store_true',
        help='Limit environments to Atari Pong environment')
    pars.add_argument(
        '--lunar',
        action='store_true',
        help='Limit environments to LunarLander environments (i.e. LunarLanderContinuous-v2, LunarLander-v2)')

def main():
    iml_logging.setup_logging()
    parser = argparse.ArgumentParser(
        textwrap.dedent("""\
        Run a bunch of experiments for a paper.
        """),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # TODO:
    # - add sub-parser for each training script
    #   - options for train_stable_baselines.sh:
    #     - run all the env-ids for ONE algorithm (investigate simulator overhead)
    #     - run all the algo-ids for ONE env-id (investigate influence of RL algorithm)
    #     - If neither of these is provides, just run EVERYTHING.
    #     - If both of these is provided, just run that algo-id / env-id combo.

    # parser.add_argument('--mode',
    #                     choices=MODES,
    #                     required=True,

    parser.add_argument(
        '--debug',
        action='store_true')
    parser.add_argument(
        '--iml-debug',
        action='store_true')
    # Don't support --pdb for iml-bench since I haven't figured out how to
    # both (1) log stdout/stderr of a command, (2) allow pdb debugger prompt
    # to be fully functional.
    # Even if we could...you probably don't want to log your pdb session
    # and all its color-codes anyway.
    # parser.add_argument(
    #     '--pdb',
    #     action='store_true')
    parser.add_argument('--debug-single-thread',
                        action='store_true',
                        help=textwrap.dedent("""
    Debug with single thread.
    """))
    parser.add_argument(
        '--iml-prof',
        default='iml-prof',
        help=textwrap.dedent("""
        Run train.py inside of iml-prof (for uninstrumented runs only)
          $ iml-prof python train.py
        This is done by setting IML_PROF=<--iml-prof> inside the train_stable_baselines.sh training script.
        
        If for some reason you didn't want to run with iml-prof, you could set this to --iml-prof="".
        """))
    parser.add_argument(
        '--replace',
        action='store_true')
    parser.add_argument(
        '--dry-run',
        action='store_true')
    parser.add_argument(
        '--skip-error',
        action='store_true',
        help=textwrap.dedent("""
    If a script fails and there's more left to run, ignore it and continue.
    (NOTE: don't worry, stderr error will still get captured to a logfile).
    """))
    parser.add_argument(
        '--analyze',
        action='store_true')
    parser.add_argument(
        '--workers',
        type=int,
        help="Number of simultaneous iml-analyze jobs; memory is the limitation here, not CPUs",
        # For some reason, I'm getting this error when selecting sql_reader.processes:
        #
        # psycopg2.OperationalError: server closed the connection unexpectedly
        # This probably means the server terminated abnormally
        # before or while processing the request.
        #
        # When restarting iml-analyze the problem doesn't re-appear.
        # I suspect this is related to running multiple jobs but it's hard to know for sure.
        # Until we figure it out, run 1 job at time.
        #
        # NOTE: Just hacked around it and reconnected when "SELECT 1" fails.
        # Still not sure about root cause.
        # I don't think it's caused by too many connections; I experienced this error after
        # a connection was left for about 8-9 minutes without being used.
        # default=2,
        #
        # I think we have memory errors now that op-events have increased...
        default=1,
    )
    parser.add_argument(
        '--dir',
        default='./output/iml_bench/all',
        help=textwrap.dedent("""\
        Directory to store stuff in for the subcommand.
        
        all subcommands: 
          log files
        train_stable_baselines.sh / stable-baselines:
          trace-files @ <algo>/<env>
        plot-stable-baselines:
          plots
        """.rstrip()))
    parser.add_argument(
        '--unins-dir',
        default='./output/expr_total_training_time',
        help=textwrap.dedent("""\
        Root directory containing full uninstrumented training runs 
        """.rstrip()))
    parser.add_argument(
        '--repetition',
        type=int,
        help=textwrap.dedent("""\
        e.g. for --repetition 1, --config instrumented
        $IML_DIR/output/iml_bench/all/config_instrumented_repetition_01
        """.rstrip()))

    parser.add_argument('--algo-env-group',
                        choices=expr_config.ALGO_ENV_GROUP_CHOICES,
                        help=textwrap.dedent("""
        Only run a specific "experiment".
        i.e. only run (algo, env) combinations needed for a specific graph.
        
        Default: run all (algo, env) combinations for all experiments.
        """))
    add_config_options(parser)

    subparsers = parser.add_subparsers(
        dest='subcommand',
        description='valid sub-commands, identified by bash training script wrapper',
        title='sub-commands',
        help='sub-command help')

    # create the parser for the "a" command

    parser_train_stable_baselines = subparsers.add_parser(
        'train_stable_baselines.sh',
        help='stable-baselines training experiments')
    add_stable_baselines_options(parser_train_stable_baselines)
    parser_train_stable_baselines.set_defaults(
        func=run_stable_baselines,
        subparser=parser_train_stable_baselines)

    parser_inference_stable_baselines = subparsers.add_parser(
        'inference_stable_baselines.sh',
        help='stable-baselines inference experiments')
    add_stable_baselines_options(parser_inference_stable_baselines)
    parser_inference_stable_baselines.set_defaults(
        func=run_stable_baselines,
        subparser=parser_inference_stable_baselines)

    parser_dummy_error = subparsers.add_parser(
        'dummy_error.sh',
        help='test: make sure errors in bench.py are handled correctly')
    parser_dummy_error.add_argument(
        '--error',
        action='store_true',
        help='If set, run a command with non-zero exit status')
    parser_dummy_error.set_defaults(
        func=run_stable_baselines,
        subparser=parser_dummy_error)

    parser_stable_baselines = subparsers.add_parser(
        'stable-baselines',
        help=textwrap.dedent("""
        iml-bench group: 
        
        Run ALL the stable-baselines experiments.
        
        If you just want to plot stuff, you can use: 
        $ iml-bench stable-baselines --mode plot
        
        See --mode for more.
        """))
    parser_stable_baselines.add_argument(
        '--mode',
        choices=['plot', 'run', 'all'],
        default='all',
        help=textwrap.dedent("""
        train_stable_baselines.sh
          Run stable-baselines experiments.
        """))
    parser_stable_baselines.set_defaults(
        func=run_group,
        subparser=parser_stable_baselines)


    args, extra_argv = parser.parse_known_args()
    # logging.info("parsed arguments: {msg}".format(
    #     msg=pprint_msg({
    #         'args': args,
    #         'extra_argv': extra_argv,
    #     })))
    os.makedirs(args.dir, exist_ok=True)

    global STABLE_BASELINES_AVAIL_ENV_IDS, STABLE_BASELINES_UNAVAIL_ENV_IDS
    STABLE_BASELINES_AVAIL_ENV_IDS, STABLE_BASELINES_UNAVAIL_ENV_IDS = expr_config.detect_available_env(expr_config.STABLE_BASELINES_ENV_IDS)
    print("Available env ids:")
    print(textwrap.indent("\n".join(STABLE_BASELINES_AVAIL_ENV_IDS), prefix='  '))
    print("Unavailable env ids:")
    print(textwrap.indent("\n".join(sorted(STABLE_BASELINES_UNAVAIL_ENV_IDS.keys())), prefix='  '))

    if args.debug:
        logging.info(pprint_msg({'args': args.__dict__}))

    args.func(parser, args, extra_argv)

def run_stable_baselines(parser, args, extra_argv):
    obj = StableBaselines(args, extra_argv)
    assert args.subcommand is not None
    obj.run(parser)

class Experiment:

    @property
    def _sub_cmd(self):
        args = self.args

        sub_cmd = args.subcommand

        m = re.search(r'(?P<sh_name>.*)\.sh$', args.subcommand)
        if m:
            sub_cmd = m.group('sh_name')

        return sub_cmd

    def already_ran(self, to_file):
        if not _e(to_file):
            return False
        with open(to_file) as f:
            for lineno, line in enumerate(f, start=1):
                line = line.rstrip()
                if re.search(r'IML BENCH DONE', line):
                    if self.args.debug:
                        logging.info("Saw \"IML BENCH DONE\" in {path} @ line {lineno}; skipping.".format(
                            lineno=lineno,
                            path=to_file))
                    return True
        return False

    def is_supported(self, algo, env_id):
        for expr in expr_config.STABLE_BASELINES_EXPRS:
            if expr.algo == algo and expr.env_id == env_id:
                return expr
        return None

    def _gather_algo_env_pairs(self, algo=None, env_id=None, bullet=False, atari=False, lunar=False, algo_env_group=None, debug=False):
        return expr_config.stable_baselines_gather_algo_env_pairs(
            algo=algo,
            env_id=env_id,
            bullet=bullet,
            atari=atari,
            lunar=lunar,
            algo_env_group=algo_env_group,
            debug=debug)

    def _run_cmd(self, cmd, to_file, env=None, replace=False, debug=False):
        args = self.args

        if env is None:
            # Make sure iml-analyze get IML_POSTGRES_HOST
            env = dict(os.environ)

        proc = None
        if ( replace or args.replace ) or not self.already_ran(to_file):

            try:
                logging.info("> Calling tee: cmd =\n  $ {cmd}".format(cmd=cmd_as_string(cmd + self.extra_argv)))
                proc = tee(
                    cmd=cmd + self.extra_argv,
                    to_file=to_file,
                    env=env,
                    dry_run=args.dry_run,
                )
                logging.info("> Done calling tee: cmd =\n  $ {cmd}".format(cmd=cmd_as_string(cmd + self.extra_argv)))
                failed = False
            except subprocess.CalledProcessError as e:
                if not args.skip_error:
                    logging.info((
                                     "> Command failed: see {path}; exiting early "
                                     "(use --skip-error to ignore individual experiment errors)"
                                 ).format(path=to_file))
                    ret = 1
                    if debug:
                        logging.info("Exiting with ret={ret}\n{stack}".format(
                            ret=ret,
                            stack=get_stacktrace(),
                        ))
                    sys.exit(ret)
                logging.info(
                    "> Command failed; see {path}; continuing (--skip-error was set)".format(
                        path=to_file,
                    ))
                failed = True

            if not failed:
                if proc is not None and proc.returncode != 0:
                    logging.info("BUG: saw returncode = {ret}, expected 0".format(
                        ret=proc.returncode))
                    assert proc.returncode == 0
                if not args.dry_run:
                    with open(to_file, 'a') as f:
                        f.write("IML BENCH DONE\n")
                if not args.dry_run:
                    assert self.already_ran(to_file)

        return proc


class ExperimentGroup(Experiment):
    def __init__(self, args, extra_argv):
        self.args = args
        self.extra_argv = extra_argv
        # self.pool = ForkedProcessPool(name="iml_analyze_pool", max_workers=args.workers, debug=self.args.debug)

    def should_run_algo_env_group(self, algo_env_group):
        return self.args.algo_env_group is None or algo_env_group == self.args.algo_env_group

    @property
    def root_iml_directory(self):
        args = self.args
        root_iml_directory = get_root_iml_directory(args.config, args.dir, args.repetition)
        return root_iml_directory

    def stable_baselines(self, parser):
        """
        To avoid running everything more than once, we will stick ALL (algo, env_id) pairs inside of $IML_DIR/output/iml_bench/all.

        (1) On vs off policy:
        Run Atari Pong on all environments that support it (that we have annotated):
        - Ppo2 [sort of on-policy]
        - A2c [on-policy]
        - Dqn [off-policy]
        We can use this to compare on-policy vs off-policy

        (2) Compare environments:
        Run ALL bullet environments for at least one algorithm (ppo2).

        (3) Compare algorithms:
        Run ALL algorithms on 1 bullet environment (Walker)

        (4) Compare all RL workloads:
        Run ALL algorithms on ALL bullet environments
        """

        args = self.args

        os.makedirs(self.root_iml_directory, exist_ok=True)

        subparser = self.args.subparser
        # Not multiprocessing friendly (cannot pickle)
        del self.args.subparser

        def bench_log(expr):
            if args.analyze:
                return "{expr}.analyze.log".format(expr=expr)
            return "{expr}.log".format(expr=expr)

        def plot_log(expr, overlap_type=None, operation=None):
            """
            {expr}.{overlap_type}.{operation}.plot
            ------
            if present.
            """
            def _add(suffix, string):
                if string is not None:
                    suffix = "{suffix}.{string}".format(suffix=suffix, string=string)
                return suffix

            suffix = expr
            suffix = _add(suffix, overlap_type)
            suffix = _add(suffix, operation)
            suffix = _add(suffix, 'plot')

            return suffix

        def rl_workloads_msg(expr, algo_env_pairs):
            all_pairs = gather.algo_env_pairs()
            return "RL workloads: algo_env_group={algo_env_group}: {msg}".format(
                algo_env_group=expr,
                msg=pprint_msg({
                    'all_pairs': all_pairs,
                    'algo_env_pairs': algo_env_pairs,
                }),
            )

        self.will_run = False
        # self.will_analyze = False
        self.will_plot = False

        if args.mode in ['all', 'run']:
            self.will_run = True

        if args.mode in ['plot'] and args.config == 'instrumented':
            self.will_plot = True

        # def _run(expr, train_stable_baselines_opts, stacked_args):
        #     self.iml_bench(parser, subparser, 'train_stable_baselines.sh', train_stable_baselines_opts, suffix=bench_log(expr))
        #     self.stacked_plot(stacked_args, train_stable_baselines_opts, suffix=plot_log(expr))


        # TODO: Run all the same experiments, BUT, under a different configuration.
        # We only want to plot stuff during the "extrapolated" configuration.

        gather = GatherAlgoEnv(args)

        # - Debug experiment:
        #   Choose a particular (algo, env) to use to debug stuff on.
        expr = 'debug_expr'
        algo = 'ppo2'
        env = 'HalfCheetahBulletEnv-v0'
        algo_env_pairs = [(algo, env)]
        opts = ['--env-id', env, '--algo', algo]
        if self.should_run_algo_env_group(expr):
            logging.info(rl_workloads_msg(expr, algo_env_pairs))
            self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr), debug=True)
            overlap_type = 'ResourceOverlap'
            self.stacked_plot([
                '--overlap-type', overlap_type,
                '--y-type', 'percent',
                '--x-type', 'algo-comparison',
                '--training-time',
            ], suffix=plot_log(expr, overlap_type), algo_env_pairs=algo_env_pairs)

        # (1) On vs off policy:
        expr = 'on_vs_off_policy'
        env = 'PongNoFrameskip-v4'
        algo_env_pairs = gather.pairs_by_env(env)
        opts = ['--env-id', env]
        if self.should_run_algo_env_group(expr):
            logging.info(rl_workloads_msg(expr, algo_env_pairs))
            self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr))
            overlap_type = 'ResourceOverlap'
            self.stacked_plot([
                '--overlap-type', overlap_type,
                '--y-type', 'percent',
                '--x-type', 'algo-comparison',
                '--training-time',
            ], suffix=plot_log(expr, overlap_type), algo_env_pairs=algo_env_pairs)

        # (2) Environment choice:
        expr = 'environment_choice'
        algo_env_pairs = gather.pairs_by_algo_env_group(expr)
        # NOTE: --algo-env-group is forwarded automatically since it's an option that belongs to the "main" parser, not the subparser
        opts = []
        if self.should_run_algo_env_group(expr):
            logging.info(rl_workloads_msg(expr, algo_env_pairs))
            self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr))
            # overlap_type = 'OperationOverlap'
            overlap_type = 'CategoryOverlap'
            # NOTE: gather across ALL ppo directories, including environment we cannot run like AirLearning.
            algo_env_pairs = gather.pairs_by_algo('ppo2')
            # algo_env_pairs = [(algo, env) for algo, env in algo_env_pairs if re.search('AirLearning', env)]
            self.stacked_plot([
                '--overlap-type', overlap_type,

                # '--resource-overlap', json.dumps(['CPU']),
                # '--remap-df', json.dumps([textwrap.dedent("""
                #         # Replace ppo2 operations with simple (Inference, Simulation, Backpropagation) labels
                #         # established in Background discussion of typical RL workloads.
                #         # Sanity check to make sure we don't forget any operations.
                #         assert regions.issubset({
                #             ('step',),
                #             ('sample_action',),
                #             ('compute_advantage_estimates',),
                #             ('optimize_surrogate',),
                #             ('training_loop',),
                #         })
                #         new_df[('Simulation',)] = df[('step',)]
                #         new_df[('Inference',)] = df[('sample_action',)]
                #         new_df[('Backpropagation',)] = df[('compute_advantage_estimates',)] + df[('optimize_surrogate',)] + df[('training_loop',)]
                #         """)]),
                # '--training-time',
                # # '--extrapolated-training-time',

                '--y-lim-scale-factor', 1.10,
                '--detailed',
                '--rotation', 15,
                '--remap-df', json.dumps([textwrap.dedent("""
                        # Replace ppo2 operations with simple (Inference, Simulation, Backpropagation) labels
                        # established in Background discussion of typical RL workloads.
                        def ppo_pretty_operation(op_name):
                            if op_name in {'compute_advantage_estimates', 'optimize_surrogate', 'training_loop'}:
                                return "Backpropagation"
                            elif op_name == 'sample_action':
                                return "Inference"
                            elif op_name == 'step':
                                return "Simulation"
                            else:
                                raise NotImplementedError("Not sure what pretty-name to use for op_name={op_name}".format(
                                    op_name=op_name))
                        new_df['operation'] = new_df['operation'].apply(ppo_pretty_operation)
                        """)]),

                '--y-type', 'percent',
                '--x-type', 'env-comparison',
                # '--y2-logscale',
            ], suffix=plot_log(expr, overlap_type), algo_env_pairs=algo_env_pairs)

        # (3) Algorithm choice:
        # Want to show on-policy vs off-policy.
        # e.g. compare DDPG against PPO
        # We want to show that:
        # - 'step' time of DDPG vs PPO; 'step' of DDPG should be bigger for SAME environment
        # - Q: this info comes from OperationOverlap CPU-breakdown; each algo definitely has a step annotation;
        #   however currently the algorithms have other annotations that are inconsistent;
        # - So, this feature depends on region-merging.
        # - HOWEVER, we DON'T need to re-run for this one.
        # - region-map:
        #     step = step
        #     other = sum(r for r in regions if r != step)

        # (4) DQN detailed category/operation breakdown plot:
        expr = 'dqn_detailed'
        # algo_env_pairs = [('dqn', 'PongNoFrameskip-v4')]
        algo_env_pairs = gather.pairs_by_algo_env_group(expr)
        # NOTE: --algo-env-group is forwarded automatically since it's an option that belongs to the "main" parser, not the subparser
        opts = []

        if self.should_run_algo_env_group(expr):
            logging.info(rl_workloads_msg(expr, algo_env_pairs))
            self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr))
            overlap_type = 'CategoryOverlap'
            self.stacked_plot([
                '--detailed',
                '--overlap-type', overlap_type,
                '--remap-df', json.dumps([textwrap.dedent("""\
                    # Replace operations with simple (Inference, Simulation, Backpropagation) labels
                    # established in Background discussion of typical RL workloads.
                    def dqn_pretty_operation(op_name):
                        if op_name in {'q_backward', 'q_update_target_network', 'training_loop'}:
                            return "Backpropagation"
                        elif op_name == 'q_forward':
                            return "Inference"
                        elif op_name == 'step':
                            return "Simulation"
                        else:
                            raise NotImplementedError("Not sure what pretty-name to use for op_name={op_name}".format(
                                op_name=op_name))
                    new_df['operation'] = new_df['operation'].apply(dqn_pretty_operation)
                        """)]),
                '--y-type', 'percent',
                '--x-type', 'rl-comparison',
                # '--training-time',
                # '--y2-logscale',
            ], suffix=plot_log(expr, overlap_type), algo_env_pairs=algo_env_pairs)

        def plot_expr_algorithm_choice(expr, algo_env_pairs):
            """
            Map all algorithm operations to uniform, simplified labels:
            - Inference: choosing an action to perform by consulting a neural network
            - Simulation: interacting with a simulator
            - Backpropagation: forward/backward pass, update network, maintain target network

            A2C:
                Inference = sample_action
                Simulation = step
                Backpropagation = train_step + training_loop

            DDPG:
                Inference = sample_action
                Simulation = step
                Backpropagation = evaluate + train_step + training_loop + update_target_network
                ...seems like evaluate may belong to inference...not sure though...
                IDEA: just delete / ignore it... these are RAW times so we CAN do that....HOWEVER training time still includes it so its WRONG to show that.

            PPO2:
                Inference = sample_action
                Simulation = step
                Backpropagation = compute_advantage_estimates + optimize_surrogate + training_loop

            SAC:
                Inference = sample_action
                Simulation = step
                Backpropgation = training_loop + update_actor_and_critic + update_target_network
            """
            if self.should_run_algo_env_group(expr):
                # overlap_type = 'OperationOverlap'
                overlap_type = 'CategoryOverlap'
                self.stacked_plot([
                    '--overlap-type', overlap_type,

                    '--detailed',
                    '--remap-df', json.dumps([textwrap.dedent("""
                    def pretty_operation(algo, op_name):
                        if algo == 'ppo2':
                            if op_name in {'compute_advantage_estimates', 'optimize_surrogate', 'training_loop'}:
                                return "Backpropagation"
                            elif op_name == 'sample_action':
                                return "Inference"
                            elif op_name == 'step':
                                return "Simulation"
                        elif algo == 'ddpg':
                            if op_name in {'evaluate', 'train_step', 'training_loop', 'update_target_network'}:
                                return "Backpropagation"
                            elif op_name == 'sample_action':
                                return "Inference"
                            elif op_name == 'step':
                                return "Simulation"
                        elif algo == 'a2c':
                            if op_name in {'train_step', 'training_loop'}:
                                return "Backpropagation"
                            elif op_name == 'sample_action':
                                return "Inference"
                            elif op_name == 'step':
                                return "Simulation"
                        elif algo == 'sac':
                            if op_name in {'training_loop', 'update_actor_and_critic', 'update_target_network'}:
                                return "Backpropagation"
                            elif op_name == 'sample_action':
                                return "Inference"
                            elif op_name == 'step':
                                return "Simulation"
                        raise NotImplementedError("Not sure what pretty-name to use for algo={algo}, op_name={op_name}".format(
                            algo=algo,
                            op_name=op_name))
                    new_df['operation'] = np.vectorize(pretty_operation, otypes=[str])(new_df['algo'], new_df['operation'])
                        """)]),

                    # '--resource-overlap', json.dumps(['CPU']),
                    # '--training-time',
                    # '--remap-df', json.dumps([textwrap.dedent("""
                    #     # Categorize ALL RL algorithms into simplified categories:
                    #     #   Inference, Simulation, Backpropgation
                    #     import pprint
                    #     pprint.pprint({
                    #         'algorithm_choice.remap_df.original_df': df,
                    #         'regions': regions,
                    #         '(algo, env)': "({algo}, {env})".format(algo=algo, env=env),
                    #     })
                    #     # Keep 'step' region
                    #     if algo == 'ppo2':
                    #         # Inference = sample_action
                    #         # Simulation = step
                    #         # Backpropagation = compute_advantage_estimates + optimize_surrogate + training_loop
                    #         assert regions.issubset({
                    #             ('step',),
                    #             ('sample_action',),
                    #             ('compute_advantage_estimates',),
                    #             ('optimize_surrogate',),
                    #             ('training_loop',),
                    #         })
                    #         new_df[('Simulation',)] = df[('step',)]
                    #         new_df[('Inference',)] = df[('sample_action',)]
                    #         new_df[('Backpropagation',)] = df[('compute_advantage_estimates',)] + df[('optimize_surrogate',)] + df[('training_loop',)]
                    #     elif algo == 'ddpg':
                    #         # Inference = sample_action
                    #         # Simulation = step
                    #         # Backpropagation = evaluate + train_step + training_loop + update_target_network
                    #         assert regions.issubset({
                    #             ('step',),
                    #             ('sample_action',),
                    #             ('evaluate',),
                    #             ('train_step',),
                    #             ('training_loop',),
                    #             ('update_target_network',),
                    #         })
                    #         new_df[('Simulation',)] = df[('step',)]
                    #         new_df[('Inference',)] = df[('sample_action',)]
                    #         new_df[('Backpropagation',)] = df[('evaluate',)] + df[('train_step',)] + df[('training_loop',)] + df[('update_target_network',)]
                    #     elif algo == 'a2c':
                    #         # Inference = sample_action
                    #         # Simulation = step
                    #         # Backpropagation = train_step + training_loop
                    #         assert regions.issubset({
                    #             ('step',),
                    #             ('sample_action',),
                    #             ('train_step',),
                    #             ('training_loop',),
                    #         })
                    #         new_df[('Simulation',)] = df[('step',)]
                    #         new_df[('Inference',)] = df[('sample_action',)]
                    #         new_df[('Backpropagation',)] = df[('train_step',)] + df[('training_loop',)]
                    #     elif algo == 'sac':
                    #         # Inference = sample_action
                    #         # Simulation = step
                    #         # Backpropgation = training_loop + update_actor_and_critic + update_target_network
                    #         assert regions.issubset({
                    #             ('step',),
                    #             ('sample_action',),
                    #             ('update_actor_and_critic',),
                    #             ('update_target_network',),
                    #             ('training_loop',),
                    #         })
                    #         new_df[('Simulation',)] = df[('step',)]
                    #         new_df[('Inference',)] = df[('sample_action',)]
                    #         new_df[('Backpropagation',)] = df[('training_loop',)] + df[('update_actor_and_critic',)] + df[('update_target_network',)]
                    #     else:
                    #         raise NotImplementedError("Not sure how to remap operation labels {labels} into simplified labels=[Inference, Simulation, Backpropgation] for algo={algo}".format(labels=regions, algo=algo))
                    #     """)]),

                    '--y-type', 'percent',
                    '--x-type', 'algo-comparison',
                ], suffix=plot_log(expr, overlap_type), algo_env_pairs=algo_env_pairs)

        # env_id = 'Walker2DBulletEnv-v0'
        expr = 'algorithm_choice_1a_med_complexity'
        algo_env_pairs = gather.pairs_by_algo_env_group(expr)
        opts = []
        if self.should_run_algo_env_group(expr):
            logging.info(rl_workloads_msg(expr, algo_env_pairs))
            self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr))
        plot_expr_algorithm_choice(expr=expr, algo_env_pairs=algo_env_pairs)

        expr = 'algorithm_choice_1b_low_complexity'
        algo_env_pairs = gather.pairs_by_algo_env_group(expr)
        opts = []
        if self.should_run_algo_env_group(expr):
            logging.info(rl_workloads_msg(expr, algo_env_pairs))
            self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr))
        plot_expr_algorithm_choice(expr=expr, algo_env_pairs=algo_env_pairs)

        # (4) Compare all RL workloads:
        expr = 'all_rl_workloads'
        algo_env_pairs = gather.pairs_by_algo_env_group(expr)
        opts = []
        common_dims = [
            '--width', '16',
            '--height', '6',
            '--rotation', '45',
        ]
        rl_workload_dims = [
            '--show-legend', 'False',
        ]
        if self.should_run_algo_env_group(expr):
            logging.info(rl_workloads_msg(expr, algo_env_pairs))
            # self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr))
            # # - Statement: GPU utilization is low across all workloads.
            # # - Plot: ResourceOverlap that compares [CPU] vs [CPU + GPU] breakdown
            # # Need 2 plots
            # # - 1st plot shows GPU time spent is tiny
            # #   TODO: we want categories to be 'CPU', 'CPU + GPU', 'GPU' (ResourceOverlap)
            # overlap_type = 'ResourceOverlap'
            algo_env_pairs = self.algo_env_pairs(include_minigo=True, debug=True)
            # self.stacked_plot([
            #     '--overlap-type', overlap_type,
            #     '--y-type', 'percent',
            #     '--training-time',
            #     '--y2-logscale',
            # ] + rl_workload_dims + common_dims, suffix=plot_log(expr, overlap_type), algo_env_pairs=algo_env_pairs)

            self.util_plot([
                '--algo-env-from-dir',
            ], plot_args=[
                '--y-title', 'GPU utilization (%)',
                         ] + common_dims, suffix=plot_log(expr), algo_env_pairs=algo_env_pairs)

            # - 2nd plot shows where CPU time is going
            #   TODO: we want categories to be 'C++ framework', 'CUDA API C', 'Python' (CategoryOverlap)
            #   TODO: For WHAT GPU operation...? We need to merge everyone into an 'Inference' category first.
            overlap_type = 'CategoryOverlap'
            # PROBLEM: sample_action is q_forward for DQN... but that means it appears in an entirely separate file.
            # What we'd like to do:
            # - read the q_forward CategoryOverlap file.
            # - IF operation == q_forward, THEN remap it to sample_action.
            # - SOLUTION:
            #   - allow specifying multiple selector's; currently we can only specify 1 selector.
            #     e.g. --selectors "[{selector-1}, {selector-2}]"
            # - For Sri meeting:
            #   - remove DQN for now.
            #   - fix janky plot x-axis alignment
            # TODO: keep dqn and use --selectors to select different files and remap-df to remap dqn[q_forward] to dqn[sample_action]

            # algo_env_pairs = [(algo, env) for algo, env in algo_env_pairs if not re.search(r'dqn', algo)]
            # gpu_operations = ['sample_action']
            # for gpu_operation in gpu_operations:
            #     self.stacked_plot([
            #         '--overlap-type', overlap_type,
            #         '--resource-overlap', json.dumps(['CPU']),
            #         '--operation', gpu_operation,
            #         # '--selectors', gpu_operation,
            #         '--training-time',
            #         '--remap-df', json.dumps([textwrap.dedent("""
            #             keep_regions = [
            #                 ('Python',),
            #                 ('Framework API C',),
            #             ]
            #             for r in keep_regions:
            #                 new_df[r] = df[r]
            #             new_df[('CUDA API CPU',)] = df[('CUDA API CPU', 'Framework API C',)]
            #         """)]),
            #
            #
            #         # '--remap-df', json.dumps([textwrap.dedent("""
            #         # # Inference:
            #         # #   - operations we would use when deploying the trained model in production
            #         # # TODO:
            #         # # Backward-pass:
            #         # #   - additional operations required when training, but NOT from a fully trained model.
            #         # # Weight-update:
            #         # #   - after gradients are computed, update the weights
            #         # inference_ops = set([('step',), ('sample_action',)])
            #         # other_ops = set([op for op in regions if op not in inference_ops])
            #         # import ipdb; ipdb.set_trace()
            #         # new_df[('inference',)] = np.sum(df[inference_ops])
            #         # new_df[('other',)] = np.sum(df[other_ops])
            #         # """)]),
            #
            #         '--y-type', 'percent',
            #     ] + rl_workload_dims + common_dims, suffix=plot_log(expr, overlap_type, gpu_operation), algo_env_pairs=algo_env_pairs)

        # - Statement: GPU operations are dominated by CPU time. In particular, CUDA API C
        #   time is a dominant contributor.
        # - Plot: There are multiple GPU operations, and for each one we would have a separate plot...?
        #   Well, we COULD stick them all in one plot, so long as the operations don't overlap.
        #   I need to be careful about inference is in the case of minigo.
        #   Inference - Forward-pass: CategoryOverlap, [CPU, GPU]
        #   Training - Backward-pass: CategoryOverlap, [CPU, GPU]
        #   Q: Should we include weight-update here?
        # - IDEALLY: we need the generic breakdown; for now we can do "Inference", since
        #   every algorithm I collected at least has "sample_action"
        # - NOTE: it would be nice to have GPU-time in this plot as well...
        #   currently I don't think that's easy to do since it would involve "merging"
        #   two *.venn_js.json files, one from ['CPU', 'GPU'] and one from ['CPU']

        # # (5) Different RL algorithms on the industrial-scale simulator: AirLearning environment:
        # expr = 'beefy_simulator'
        # # opts = ['--bullet', '--algo', 'ppo2']
        # # self.iml_bench(parser, subparser, 'train_stable_baselines.sh', opts, suffix=bench_log(expr))
        # overlap_type = 'OperationOverlap'
        # iml_dirs  = self.iml_dirs_air_learning()
        # self.stacked_plot([
        #     '--overlap-type', overlap_type,
        #     '--resource-overlap', json.dumps(['CPU']),
        #     '--y-type', 'percent',
        #     '--x-type', 'algo-comparison',
        # ], suffix=plot_log(expr, overlap_type), iml_dirs=iml_dirs)

    def algos(self):
        return set(algo for algo, env in self.algo_env_pairs())

    def envs(self):
        return set(env for algo, env in self.algo_env_pairs())

    def iml_directory(self, algo, env_id):
        args = self.args
        iml_directory = get_iml_directory(args.config, args.dir, algo, env_id, args.repetition)
        return iml_directory

    def unins_iml_directory(self, algo, env_id):
        args = self.args
        config = 'uninstrumented'
        repetition = 1
        iml_directory = _j(args.unins_dir, algo, env_id, 'config_{config}_repetition_{r:02}'.format(
            config=config,
            r=repetition,
        ))
        return iml_directory

    def algo_env_pairs(self, include_minigo=False, debug=False):
        gather = GatherAlgoEnv(self.args, include_minigo=include_minigo)
        return gather.algo_env_pairs(debug=debug)

    def _is_air_learning_env(self, env):
        return re.search(r'AirLearning', env)

    def iml_dirs_air_learning(self, debug=False):
        algo_env_pairs = [(algo, env) for algo, env in self.algo_env_pairs() \
                          if self._is_air_learning_env(env)]
        logging.info(pprint_msg({
            'self.algo_env_pairs()': self.algo_env_pairs(),
            'algo_env_pairs': algo_env_pairs}))
        iml_dirs = [self.iml_directory(algo, env) for algo, env in algo_env_pairs]
        return iml_dirs

    def stacked_plot(self, stacked_args, suffix, algo_env_pairs=None, debug=False):
        if not self.will_plot:
            return
        args = self.args
        cmd = [
            'iml-analyze',
        ]
        cmd.extend([
            '--task', 'OverlapStackedBarTask',
        ])

        if args.debug:
            cmd.append('--debug')
        # if args.pdb:
        #     cmd.append('--pdb')

        if algo_env_pairs is None:
            algo_env_pairs = []
        algo_env_pairs = list(algo_env_pairs)

        # if algo_env_pairs is None:
        #     raise NotImplementedError("Not sure what to use for --iml-directories")
        if len(algo_env_pairs) == 0 and not self.args.dry_run:
            raise NotImplementedError("Need at least one directory for --iml-directories but saw 0.")
        def sort_key(algo_env):
            """
            Show bar graphs order by algo first, then environment.
            """
            algo, env = algo_env
            return (algo, env)
        # Remove duplicates.
        algo_env_pairs = list(set(algo_env_pairs))
        algo_env_pairs.sort(key=sort_key)
        iml_dirs = [self.iml_directory(algo, env_id) for algo, env_id in algo_env_pairs]
        unins_iml_dirs = []
        for algo, env_id in algo_env_pairs:
            unins_iml_dir = self.unins_iml_directory(algo, env_id)
            if os.path.isdir(unins_iml_dir):
                unins_iml_dirs.append(unins_iml_dir)
            else:
                logging.info("OverlapStackedBarTask.SKIP unins_iml_dir = {path}".format(
                    path=unins_iml_dir))
        logging.info("OverlapStackedBarTask.args =\n{msg}".format(
            msg=pprint_msg({
                'algo_env_pairs': algo_env_pairs,
                'unins_iml_dirs': unins_iml_dirs,
            })))
        cmd.extend([
            '--iml-directories', json.dumps(iml_dirs),
            '--unins-iml-directories', json.dumps(unins_iml_dirs),
        ])

        cmd.extend([
            # Output directory for the png plots.
            '--directory', self.root_iml_directory,
            # Add expr-name to png.
            '--suffix', suffix,
        ])

        cmd.extend(stacked_args)

        cmd.extend(self.extra_argv)

        to_file = self._get_logfile(suffix="{suffix}.log".format(suffix=suffix))

        self._run_cmd(cmd=cmd, to_file=to_file, replace=True)

    def util_plot(self, stacked_args, suffix, plot_args=None, algo_env_pairs=None, debug=False):
        if not self.will_plot:
            return

        args = self.args

        def _util_csv(algo_env_pairs):
            util_task_cmd = [
                'iml-analyze',
            ]
            util_task_cmd.extend([
                '--task', 'UtilTask',
            ])

            if args.debug:
                util_task_cmd.append('--debug')

            if algo_env_pairs is None:
                algo_env_pairs = []
            algo_env_pairs = list(algo_env_pairs)

            if len(algo_env_pairs) == 0 and not self.args.dry_run:
                raise NotImplementedError("Need at least one directory for --iml-directories but saw 0.")
            def sort_key(algo_env):
                """
                Show bar graphs order by algo first, then environment.
                """
                algo, env = algo_env
                return (algo, env)
            # Remove duplicates.
            algo_env_pairs = list(set(algo_env_pairs))
            algo_env_pairs.sort(key=sort_key)
            iml_dirs = [self.iml_directory(algo, env_id) for algo, env_id in algo_env_pairs]
            util_task_cmd.extend([
                '--iml-directories', json.dumps(iml_dirs),
            ])

            util_task_cmd.extend([
                # Output directory for the png plots.
                '--directory', self.root_iml_directory,
                # Add expr-name to png.
                '--suffix', suffix,
            ])

            util_task_cmd.extend(stacked_args)

            util_task_cmd.extend(self.extra_argv)

            to_file = self._get_logfile(suffix="{suffix}.log".format(suffix=suffix))

            self._run_cmd(cmd=util_task_cmd, to_file=to_file, replace=True)

        def _util_plot():
            util_plot_cmd = [
                'iml-analyze',
                '--task', 'UtilPlotTask',
            ]
            if args.debug:
                util_plot_cmd.append('--debug')
            util_csv = _j(self.root_iml_directory, "overall_machine_util.raw.csv")
            util_plot_cmd.extend([
                '--csv', util_csv,
                # Output directory for the png plots.
                '--directory', self.root_iml_directory,
                # Add expr-name to png.
                '--suffix', suffix,
            ])
            if plot_args is not None:
                util_plot_cmd.extend(plot_args)
            if not _e(util_csv):
                logging.info("SKIP UtilTaskPlot; {path} doesn't exist")
                return
            to_file = self._get_logfile(suffix="UtilPlot.{suffix}.log".format(suffix=suffix))
            self._run_cmd(cmd=util_plot_cmd, to_file=to_file, replace=True)

        _util_csv(algo_env_pairs)
        _util_plot()

    def iml_bench(self, parser, subparser, subcommand, subcmd_args, suffix='log', env=None, debug=False):
        args = self.args
        if not self.will_run:
            return
        main_cmd = self._get_main_cmd(parser, subparser, subcommand)
        cmd = main_cmd + subcmd_args
        to_file = self._get_logfile(suffix=suffix)
        logging.info("Logging iml-bench to file {path}".format(path=to_file))
        self._run_cmd(cmd=cmd, to_file=to_file, env=env, debug=debug)

    def _get_main_cmd(self, parser, subparser, subcommand):
        args = self.args
        cmd_args = copy.copy(args)
        cmd_args.subcommand = subcommand
        main_cmd = args_to_cmdline(parser, cmd_args,
                                   subparser=subparser,
                                   subparser_argname='subcommand',
                                   use_pdb=False,
                                   ignore_unhandled_types=True,
                                   ignore_argnames=['func', 'subparser', 'mode', 'expr'],
                                   debug=args.debug)
        return main_cmd

    def run(self, parser):
        args = self.args
        if args.subcommand == 'stable-baselines':
            self.stable_baselines(parser)
        else:
            raise NotImplementedError

    def _get_logfile(self, suffix='log'):
        args = self.args

        to_file = _j(self.root_iml_directory, '{sub}.{suffix}'.format(
            sub=self._sub_cmd,
            suffix=suffix,
        ))
        return to_file

def run_group(parser, args, extra_argv):
    obj = ExperimentGroup(args, extra_argv)
    assert args.subcommand is not None
    obj.run(parser)


class StableBaselines(Experiment):
    def __init__(self, args, extra_argv):
        # self.parser = parser
        self.args = args
        self.extra_argv = extra_argv
        self.pool = ForkedProcessPool(name="iml_analyze_pool", max_workers=args.workers,
                                      # debug=self.args.debug,
                                      )

    def _analyze(self, algo, env_id):
        args = self.args

        iml_directory = self.iml_directory(algo, env_id)
        cmd = ['iml-analyze', "--iml-directory", iml_directory]

        to_file = self._get_logfile(algo, env_id, suffix='analyze.log')
        logging.info("Analyze logfile = {path}".format(path=to_file))

        self._run_cmd(cmd=cmd, to_file=to_file)

    def _error(self):
        args = self.args

        if args.error:
            cmd = ['ls', "-z"]
        else:
            cmd = ['ls', "-l"]

        to_file = _j(self.root_iml_directory, 'error.txt')

        self._run_cmd(cmd=cmd, to_file=to_file)

    def _config_opts(self):
        args = self.args

        opts = []

        # if not config_is_full(args.config):
        #     # If we DON'T want to run for the full training duration add --iml-trace-time-sec
        #     pass

        # "Instrumented, no tfprof"
        # "Instrumented, no pyprof"

        # TODO: I suspect this config-dir names will get overloaded fast...need to use iml_config.json file that stores
        # Profiler.attrs instead.  Technically, we should store this in the process directory...
        # {
        #   'disable_tfprof'
        # }
        # 'config_instrumented_no_tfprof'
        # 'config_instrumented_no_pyprof'

        if config_is_uninstrumented(args.config):
            # If we want to run uninstrumented, add --iml-disable, but still record training progress
            opts.extend(['--iml-disable', '--iml-training-progress'])

        if config_is_no_tfprof(args.config):
            opts.extend(['--iml-disable-tfprof'])

        if config_is_no_pyprof(args.config):
            opts.extend(['--iml-disable-pyprof'])

        if config_is_no_pydump(args.config):
            opts.extend(['--iml-disable-pyprof-dump'])

        if config_is_no_pytrace(args.config):
            opts.extend(['--iml-disable-pyprof-trace'])

        if config_is_no_tfdump(args.config):
            opts.extend(['--iml-disable-tfprof-dump'])

        return opts

    def _get_logfile(self, algo, env_id, suffix='log'):
        args = self.args

        to_file = _j(self.root_iml_directory, '{sub}.algo_{algo}.env_id_{env_id}.{suffix}'.format(
            sub=self._sub_cmd,
            algo=algo,
            env_id=env_id,
            suffix=suffix,
        ))
        return to_file

    @property
    def root_iml_directory(self):
        args = self.args
        root_iml_directory = get_root_iml_directory(args.config, args.dir, args.repetition)
        return root_iml_directory

    def iml_directory(self, algo, env_id):
        args = self.args
        iml_directory = get_iml_directory(args.config, args.dir, algo, env_id, args.repetition)
        return iml_directory

    def _sh_env(self, algo, env_id):
        args = self.args

        env = dict(os.environ)
        env['ENV_ID'] = env_id
        env['ALGO'] = algo
        if args.debug:
            env['DEBUG'] = 'yes'
            env['IML_DEBUG'] = 'yes'

        if args.iml_prof:
            env['IML_PROF'] = args.iml_prof

        return env

    def _run(self, algo, env_id):
        args = self.args

        # NOTE: We use absolute path of iml-directory
        # since some training scripts change cd to a
        # different directory before they run.
        iml_directory = _a(self.iml_directory(algo, env_id))
        cmd = [args.subcommand, "--iml-directory", iml_directory]
        if args.iml_debug:
            cmd.append('--iml-debug')
        config_opts = self._config_opts()
        cmd.extend(config_opts)

        env = self._sh_env(algo, env_id)

        to_file = self._get_logfile(algo, env_id, suffix='log')

        self._run_cmd(cmd=cmd, to_file=to_file, env=env)


    def run(self, parser):
        args = self.args

        os.makedirs(self.root_iml_directory, exist_ok=True)

        subparser = self.args.subparser
        # Not multiprocessing friendly (cannot pickle)
        del self.args.subparser

        # if args.debug:
        # logging.info(pprint_msg({'StableBaselines.args': args.__dict__}))

        if args.subcommand == 'dummy_error.sh':
            self._error()
            return

        if args.env_id is not None and args.env_id in STABLE_BASELINES_UNAVAIL_ENV_IDS:
            print("ERROR: env_id={env} is not available since gym.make('{env}') failed.".format(
                env=args.env_id))
            sys.exit(1)

        if args.analyze and config_is_uninstrumented(args.config):
            logging.info(("Cannot run iml-analyze on --config={config}; config must be instrumented "
                          "(e.g. --config instrumented), otherwise there are no IML traces to process.").format(
                config=args.config,
            ))
            sys.exit(1)


        if args.analyze:
            gather = GatherAlgoEnv(args)
            algo_env_pairs = gather.pairs_by_args()
            # if args.debug:
            logging.info("Run --analyze over (algo, env) pairs: {msg}".format(
                msg=pprint_msg({
                    'algo_env_pairs': algo_env_pairs,
                })
            ))
            for algo, env_id in algo_env_pairs:
                # self._analyze(algo, env_id)
                self.pool.submit(
                    'iml-analyze --iml-directory {iml}'.format(iml=self.iml_directory(algo, env_id)),
                    self._analyze,
                    algo, env_id,
                    sync=self.args.debug_single_thread)
            # Wait for processes to finish.
            # If we forget to do this, even if a child exits with nonzero status, we won't see it!
            try:
                self.pool.shutdown()
            except FailedProcessException as e:
                # Child has already printed its error message; just forward the exit code.
                assert e.exitcode != 0
                sys.exit(e.exitcode)
        else:
            # Gather (algo, env) pairs whose environments we can run.
            # (GatherAlgoEnv only looks at existing runs in output/all/iml_bench)
            algo_env_pairs = self._gather_algo_env_pairs(
                algo=args.algo,
                env_id=args.env_id,
                bullet=args.bullet,
                atari=args.atari,
                lunar=args.lunar,
                algo_env_group=args.algo_env_group,
            )
            if algo_env_pairs is None:
                logging.info('Please provide either --env-id or --algo')
                sys.exit(1)

            if args.debug:
                logging.info({'algo_env_pairs': algo_env_pairs, 'args.analyze': args.analyze})

            logging.info("Collect trace-data over (algo, env) pairs: {msg}".format(
                msg=pprint_msg({
                    'algo_env_pairs': algo_env_pairs,
                })
            ))
            for algo, env_id in algo_env_pairs:
                self._run(algo, env_id)


def get_root_iml_directory(config, direc, repetition):
    # if config == 'instrumented':
    #     # The default run --config.
    #     # Run IML for 2 minutes and collect full traces.
    #     iml_directory = direc
    # else:
    #     # Either 'instrumented' or 'uninstrumented'.
    #     # Add a --config sub-directory.
    #     config_dir = "config_{config}".format(config=config)
    #     iml_directory = _j(direc, config_dir)
    if repetition is not None:
        config_dir = "config_{config}_repetition_{r:02}".format(
            config=config,
            r=repetition)
    else:
        config_dir = "config_{config}".format(
            config=config)
    iml_directory = _j(direc, config_dir)
    return iml_directory

def get_iml_directory(config, direc, algo, env_id, repetition):
    root_iml_directory = get_root_iml_directory(config, direc, repetition)
    iml_directory = _j(root_iml_directory, algo, env_id)
    return iml_directory

class GatherAlgoEnv:
    def __init__(self, args, include_minigo=False):
        self.args = args
        self.include_minigo = include_minigo

    def should_skip_env(self, env):
        # For some reason, the simulation time for Humanoid is huge when trained with sac.
        # Until we figure out WHY, let's just leave it out of all the plots.
        # return re.search('Humanoid', env)
        return False

    def should_skip_algo_env(self, algo, env):
        return not expr_config.is_paper_env(algo, env)

    def should_skip_algo(self, algo):
        # For some reason, AirLearningEnv is missing annotations from the sac algorthim.
        # I suspect sac is the "cuplrit" for various issues.
        # return re.search('sac', algo)
        return False

    def _is_env_dir(self, path):
        return os.path.isdir(path) and re.search('Env|-v\d+$', path)

    def _is_algo_dir(self, path):
        return os.path.isdir(path)

    def algo_env_pairs(self, has_machine_util=False, debug=False):
        args = self.args
        algo_env_pairs = []
        # if debug:
        #     import ipdb; ipdb.set_trace()
        algo_paths = glob(_j(self.root_iml_directory, '*'))
        for algo_path in algo_paths:
            if is_config_dir(algo_path):
                continue
            if not self._is_algo_dir(algo_path):
                if args.debug:
                    logging.info("Skip algo_path={dir}".format(dir=algo_path))
                continue
            algo = _b(algo_path)
            if self.should_skip_algo(algo):
                continue
            env_paths = glob(_j(algo_path, '*'))
            for env_path in env_paths:
                if not self._is_env_dir(env_path):
                    if args.debug:
                        logging.info("Skip env_path={dir}".format(dir=env_path))
                    continue
                env = _b(env_path)
                if self.should_skip_env(env):
                    continue
                if self.should_skip_algo_env(algo, env):
                    if args.debug:
                        logging.info("Skip (algo={algo}, env={env}) @ {dir}".format(
                            algo=algo,
                            env=env,
                            dir=env_path))
                    continue

                if has_machine_util and not self.has_machine_util(algo, env):
                    continue

                algo_env_pairs.append((algo, env))
        return algo_env_pairs

    def pairs_by_algo(self, algo):
        return set((a, e) for a, e in self.algo_env_pairs() if a == algo)

    def pairs_by_args(self):
        args = self.args
        def should_keep(algo, env):
            # NOTE: If they don't provide ANY flags for selecting algo/env to run, we run NOTHING.
            # We no longer default to running everything (since it's a confusing bug)
            ret = ( args.atari and expr_config.is_atari_env(env) ) or \
                  ( args.lunar and expr_config.is_lunar_combo(algo, env) ) or \
                  ( args.bullet and expr_config.is_bullet_env(env) ) or \
                  ( args.algo_env_group is not None and expr_config.is_algo_env_group_combo(args.algo_env_group, algo, env) )
            return ret
        return self.pairs_by_func(should_keep)

    def pairs_by_env(self, env):
        return set((a, e) for a, e in self.algo_env_pairs() if e == env)

    def pairs_by_func(self, func):
        return set((a, e) for a, e in self.algo_env_pairs() if func(a, e))

    def pairs_by_lunar(self):
        return self.pairs_by_func(expr_config.is_lunar_combo)

    def pairs_by_algo_env_group(self, algo_env_group):
        def is_algo_env(algo, env):
            return expr_config.is_algo_env_group_combo(algo_env_group, algo, env)
        return self.pairs_by_func(is_algo_env)

    # def pairs_by_is_fig_algo_comparison_low_complexity(self):
    #     return self.pairs_by_func(expr_config.is_fig_algo_comparison_low_complexity)
    #
    # def pairs_by_is_fig_algo_comparison_med_complexity(self):
    #     return self.pairs_by_func(expr_config.is_fig_algo_comparison_med_complexity)
    #
    # def pairs_by_is_fig_env_comparison(self):
    #     return self.pairs_by_func(expr_config.is_fig_env_comparison)
    #
    # def pairs_by_is_paper_env(self):
    #     return self.pairs_by_func(expr_config.is_paper_env)

    @property
    def root_iml_directory(self):
        args = self.args
        root_iml_directory = get_root_iml_directory(args.config, args.dir, args.repetition)
        return root_iml_directory

    def machine_util_files(self, algo, env):
        iml_directory = self.iml_directory(algo, env)
        return [path for path in list_files(iml_directory) if is_machine_util_file(path)]

    def has_machine_util(self, algo, env):
        machine_util_files = self.machine_util_files(algo, env)
        return len(machine_util_files) > 0

if __name__ == '__main__':
    main()
